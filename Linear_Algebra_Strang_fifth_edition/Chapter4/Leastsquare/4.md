### Proof: Equivalence of Calculus Minimization and the Normal Equation

This proof demonstrates that finding the parameters of a linear model by minimizing the sum of squared errors using calculus is equivalent to solving the normal equation.

#### 1. Problem Definition

We are trying to fit a set of `m` data points `(t_i, b_i)` to a linear model `y = C + Dt`. The goal is to find the optimal parameters `C` and `D`. The objective is to minimize the sum of squared errors (E), defined as:

$$
E(C, D) = \sum_{i=1}^{m} (b_i - (C + Dt_i))^2 = \sum_{i=1}^{m} (C + Dt_i - b_i)^2
$$

---

#### 2. Method 1: Minimization via Calculus

To find the minimum of `E`, we take the partial derivatives with respect to `C` and `D` and set them to zero.

**Partial derivative with respect to C:**

$$
\frac{\partial E}{\partial C} = \frac{\partial}{\partial C} \sum_{i=1}^{m} (C + Dt_i - b_i)^2
$$

Using the chain rule, we get:

$$
\frac{\partial E}{\partial C} = \sum_{i=1}^{m} 2(C + Dt_i - b_i) \cdot \frac{\partial}{\partial C}(C + Dt_i - b_i) = \sum_{i=1}^{m} 2(C + Dt_i - b_i) \cdot 1
$$

Setting this derivative to zero and dividing by 2:

$$
\sum_{i=1}^{m} (C + Dt_i - b_i) = 0 \quad \implies \quad mC + D\left(\sum_{i=1}^{m} t_i\right) = \sum_{i=1}^{m} b_i \quad \textbf{(1)}
$$

**Partial derivative with respect to D:**

$$
\frac{\partial E}{\partial D} = \frac{\partial}{\partial D} \sum_{i=1}^{m} (C + Dt_i - b_i)^2
$$

Using the chain rule again:

$$
\frac{\partial E}{\partial D} = \sum_{i=1}^{m} 2(C + Dt_i - b_i) \cdot \frac{\partial}{\partial D}(C + Dt_i - b_i) = \sum_{i=1}^{m} 2(C + Dt_i - b_i) \cdot t_i
$$

Setting this derivative to zero and dividing by 2:

$$
\sum_{i=1}^{m} (C + Dt_i - b_i)t_i = 0 \quad \implies \quad C\left(\sum_{i=1}^{m} t_i\right) + D\left(\sum_{i=1}^{m} t_i^2\right) = \sum_{i=1}^{m} b_i t_i \quad \textbf{(2)}
$$

The calculus method gives us a system of two linear equations for `C` and `D`.

---

#### 3. Method 2: The Normal Equation

The same problem can be expressed in linear algebra as `A\mathbf{x} \approx \mathbf{b}`, where:

$$
A = \begin{bmatrix} 1 & t_1 \\ 1 & t_2 \\ \vdots & \vdots \\ 1 & t_m \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} C \\ D \end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{bmatrix}
$$

The solution that minimizes the squared error `\|A\mathbf{x} - \mathbf{b}\|^2` is given by the normal equation:

$$
A^T A \mathbf{x} = A^T \mathbf{b}
$$

Let's compute the matrices `A^T A` and `A^T \mathbf{b}`:

$$
A^T A = \begin{bmatrix} 1 & 1 & \cdots & 1 \\ t_1 & t_2 & \cdots & t_m \end{bmatrix} \begin{bmatrix} 1 & t_1 \\ 1 & t_2 \\ \vdots & \vdots \\ 1 & t_m \end{bmatrix} = \begin{bmatrix} m & \sum t_i \\ \sum t_i & \sum t_i^2 \end{bmatrix}
$$

$$
A^T \mathbf{b} = \begin{bmatrix} 1 & 1 & \cdots & 1 \\ t_1 & t_2 & \cdots & t_m \end{bmatrix} \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{bmatrix} = \begin{bmatrix} \sum b_i \\ \sum t_i b_i \end{bmatrix}
$$

Substituting these into the normal equation gives:

$$
\begin{bmatrix} m & \sum t_i \\ \sum t_i & \sum t_i^2 \end{bmatrix} \begin{bmatrix} C \\ D \end{bmatrix} = \begin{bmatrix} \sum b_i \\ \sum t_i b_i \end{bmatrix}
$$

---

#### 4. Equivalence

If we expand the matrix equation from the normal equation method, we get two linear equations:

1.  **Row 1:** `mC + (\sum t_i)D = \sum b_i`
2.  **Row 2:** `(\sum t_i)C + (\sum t_i^2)D = \sum t_i b_i`

These are precisely the same equations **(1)** and **(2)** derived from the calculus method. Therefore, both methods produce the same system of equations and yield the same optimal values for `C` and `D`.

Q.E.D.